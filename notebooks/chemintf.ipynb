{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CheminTF Self-Contained Notebook"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T00:28:26.606797Z",
     "start_time": "2025-11-03T00:28:26.600661Z"
    }
   },
   "source": [
    "# modules/constants.py\n",
    "TEMPORAL_FEATURES_DIMENSION=  10\nSPATIAL_FEATURES_DIMENSION=  4\nTEMPORAL_EMBEDDING_DIMENSION = 64\nSPATIAL_EMBEDDING_DIMENSION = 32\nEMBEDDING_DIMENSION = SPATIAL_EMBEDDING_DIMENSION + TEMPORAL_EMBEDDING_DIMENSION\nOUTPUT_DIMENSION = 3\n\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T00:28:28.294940Z",
     "start_time": "2025-11-03T00:28:26.681649Z"
    }
   },
   "source": [
    "# modules/features.py\n",
    "import datetime\nimport math\n\nimport torch\nimport utm\n\nSCALING_FACTOR_X = 400000.0\nDELTA_SCALING_FACTOR_X = 1000.0\n\n\ndef extract_spatial_features(\n    coordinates: torch.Tensor,\n) -> torch.Tensor:\n\n    utm_list = [\n        utm.from_latlon(lat.item(), lng.item())[:2]\n        for lat, lng in coordinates\n    ]\n\n    utm_tensor = torch.tensor(utm_list, dtype=torch.float32)\n\n    deltas = torch.zeros_like(utm_tensor)\n    deltas[1:] = utm_tensor[1:] - utm_tensor[:-1]\n\n    deltas /= DELTA_SCALING_FACTOR_X\n    normalized = utm_tensor / SCALING_FACTOR_X\n\n    return torch.cat([normalized, deltas], dim=1)\n\ndef cyclic_encoding(value: torch.Tensor, period: float) -> torch.Tensor:\n    \"\"\"\n    Encode a cyclic temporal feature using sine and cosine transformations.\n\n    :param value: Tensor of feature values (e.g., hour of day).\n    :param period: The period of the cycle (e.g., 24 for hours in a day).\n    :return: Tensor of shape (N, 2) with sine and cosine encodings.\n    \"\"\"\n\n    angle = 2 * math.pi * value / period\n    return torch.stack((torch.sin(angle), torch.cos(angle)), dim=-1)\n\n\nimport torch\nimport datetime\n\ndef extract_temporal_features(timestamps: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Encode timestamps into cyclic temporal features.\n\n    Each timestamp is converted into ten features:\n    - Day of week (sin, cos)\n    - Hour of day (sin, cos)\n    - Minute of hour (sin, cos)\n    - Second of minute (sin, cos)\n    - Seconds since start\n    - Delta seconds from previous timestamp\n\n    :param timestamps: Tensor (N,) — may be on CPU or GPU.\n    :return: Tensor (N, 10) on same device as input.\n    \"\"\"\n    # ensure dtype is float64 for timestamp precision\n    assert timestamps.dtype == torch.float64, \"timestamps must be of dtype torch.float64\"\n\n    if timestamps.numel() == 0:\n        return torch.empty((0, 10), dtype=torch.float32, device=timestamps.device)\n\n    device = timestamps.device\n    timestamps = timestamps.to(\"cpu\", dtype=torch.float32)  # datetime requires CPU\n\n    # Convert to datetime objects\n    dt_list = [\n        datetime.datetime.fromtimestamp(t.item(), tz=datetime.timezone.utc)\n        for t in timestamps\n    ]\n\n    # Extract calendar components\n    day = torch.tensor([dt.weekday() for dt in dt_list], dtype=torch.float32)\n    hour = torch.tensor([dt.hour for dt in dt_list], dtype=torch.float32)\n    minute = torch.tensor([dt.minute for dt in dt_list], dtype=torch.float32)\n    second = torch.tensor([dt.second for dt in dt_list], dtype=torch.float32)\n\n    # Cyclic encodings\n    day_enc = cyclic_encoding(day, 7)\n    hour_enc = cyclic_encoding(hour, 24)\n    minute_enc = cyclic_encoding(minute, 60)\n    second_enc = cyclic_encoding(second, 60)\n\n    # Timing features (computed in CPU)\n    seconds_since_start = timestamps - timestamps[0]\n    delta_seconds = torch.zeros_like(timestamps)\n    delta_seconds[1:] = timestamps[1:] - timestamps[:-1]\n\n    # Combine all and move back to original device\n    features = torch.cat(\n        [\n            day_enc,\n            hour_enc,\n            minute_enc,\n            second_enc,\n            seconds_since_start.unsqueeze(-1),\n            delta_seconds.unsqueeze(-1),\n        ],\n        dim=1,\n    ).to(device)\n\n    # Convert to float32 on the target device\n    features = features.to(dtype=torch.float32, device=device)\n\n    return features\n\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T00:28:28.472169Z",
     "start_time": "2025-11-03T00:28:28.465276Z"
    }
   },
   "source": [
    "# modules/position.py\n",
    "import torch\nfrom torch import nn\n\n\nclass LearnedPositionalEncoding(nn.Module):\n    \"\"\"GPT-2 style learnable positional encoding.\"\"\"\n\n    def __init__(self, embedding_dim: int, max_seq_len: int):\n        super().__init__()\n        self.position_embeddings = nn.Embedding(max_seq_len, embedding_dim)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Adds learned positional encodings to the input tensor.\n        Expects x of shape [T, B, E] or [B, T, E].\n        \"\"\"\n        device = x.device\n        seq_len = x.size(0) if x.dim() == 3 else x.size(1)\n        positions = torch.arange(seq_len, device=device, dtype=torch.long)\n        pos_enc = self.position_embeddings(positions)\n        if x.shape[0] == seq_len:\n            pos_enc = pos_enc.unsqueeze(1)  # [T, 1, E]\n        else:\n            pos_enc = pos_enc.unsqueeze(0)  # [1, T, E]\n        return x + pos_enc\n\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T00:28:28.565356Z",
     "start_time": "2025-11-03T00:28:28.557387Z"
    }
   },
   "source": [
    "# modules/encoder.py\n",
    "import torch\nfrom torch import nn\n\n\nclass SpatioTemporalEncoder(nn.Module):\n    \"\"\"\n    Encodes spatial and temporal features into a combined embedding,\n    with built-in normalization to align feature scales.\n\n    Each feature dimension is standardized across the batch and time axes\n    before linear projection, ensuring consistent magnitude between\n    spatial (lat/lng) and temporal (timestamps or intervals) inputs.\n    \"\"\"\n\n    def __init__(\n        self,\n        spatial_features_dimension: int = SPATIAL_FEATURES_DIMENSION,\n        temporal_features_dimension: int = TEMPORAL_FEATURES_DIMENSION,\n        spatial_embedding_dimension: int = SPATIAL_EMBEDDING_DIMENSION,\n        temporal_embedding_dimension: int = TEMPORAL_EMBEDDING_DIMENSION,\n        eps: float = 1e-6,\n    ):\n        super().__init__()\n        self.eps = eps\n        self.spatial_fc = nn.Linear(\n            spatial_features_dimension, spatial_embedding_dimension, dtype=torch.float32\n        )\n        self.temporal_fc = nn.Linear(\n            temporal_features_dimension,\n            temporal_embedding_dimension,\n            dtype=torch.float32,\n        )\n\n    def _normalize(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Normalize each feature dimension to zero mean and unit variance.\"\"\"\n        mean = x.mean(dim=(0, 1), keepdim=True)\n        std = x.std(dim=(0, 1), keepdim=True)\n        return (x - mean) / (std + self.eps)\n\n    def forward(\n        self, spatial_input: torch.Tensor, temporal_input: torch.Tensor\n    ) -> torch.Tensor:\n        # spatial_input, temporal_input: [B, T, D]\n        spatial_normed = self._normalize(spatial_input)\n        temporal_normed = self._normalize(temporal_input)\n\n        spatial_embed = self.spatial_fc(spatial_normed)\n        temporal_embed = self.temporal_fc(temporal_normed)\n\n        return torch.cat(\n            [spatial_embed, temporal_embed], dim=-1\n        )  # [B, T, spatial_emb + temporal_emb]\n\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T00:28:28.592371Z",
     "start_time": "2025-11-03T00:28:28.586611Z"
    }
   },
   "source": [
    "# modules/output.py\n",
    "import torch\nfrom torch import nn, Tensor\n\n\nclass OutputModule(nn.Module):\n    \"\"\"\n    Output projection module.\n\n    Projects the transformer hidden representations into the final output space.\n    \"\"\"\n\n    def __init__(self, embed_dim: int = EMBEDDING_DIMENSION):\n        \"\"\"\n        :param embed_dim: Dimensionality of transformer embeddings before projection.\n        \"\"\"\n        super().__init__()\n        self.output_proj = nn.Linear(embed_dim, OUTPUT_DIMENSION, dtype=torch.float32)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"\n        :param x: Input tensor of shape [T, B, E], representing hidden states\n                  from the transformer encoder, where\n                  T = sequence length,\n                  B = batch size,\n                  E = embedding dimension.\n        :return: Tensor of shape [T, B, OUTPUT_DIMENSION], representing projected outputs.\n        \"\"\"\n        return self.output_proj(x)\n\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# modules/model.py\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "\n",
    "def causal_mask(seq_len: int) -> Tensor:\n",
    "    # Generates an upper-triangular matrix filled with -inf above the diagonal\n",
    "    # Used to mask future tokens in causal (autoregressive) transformers\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1, float(\"-inf\"))\n",
    "    return mask\n",
    "\n",
    "\n",
    "class CheminTF(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_heads: int,\n",
    "        num_layers: int,\n",
    "        embed_dim: int = EMBEDDING_DIMENSION,\n",
    "        max_len: int = 50,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_encoder = SpatioTemporalEncoder(\n",
    "            spatial_features_dimension=SPATIAL_FEATURES_DIMENSION,\n",
    "            temporal_features_dimension=TEMPORAL_FEATURES_DIMENSION,\n",
    "            spatial_embedding_dimension=SPATIAL_EMBEDDING_DIMENSION,\n",
    "            temporal_embedding_dimension=TEMPORAL_EMBEDDING_DIMENSION,\n",
    "        )\n",
    "\n",
    "        self.pos_encoder = LearnedPositionalEncoding(\n",
    "            embedding_dim=EMBEDDING_DIMENSION, max_seq_len=max_len\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            # activation => we want some non-linearity here but we want to keep negative values\n",
    "            activation=\"gelu\",\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=4 * embed_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=False,\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.output = OutputModule(embed_dim=embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, spatial_input: torch.Tensor, temporal_input: torch.Tensor):\n",
    "        # x: [T, B, D]\n",
    "        x = self.input_encoder(spatial_input, temporal_input)\n",
    "        # x: [T, B, D]\n",
    "        T, B, D = x.shape\n",
    "        x = self.pos_encoder(x)  # [T, B, E]\n",
    "        mask = causal_mask(T).to(x.device)  # [T, T]\n",
    "\n",
    "        x = self.transformer(x, mask=mask)  # [T, B, E]\n",
    "\n",
    "        return self.output(x)\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# synthetic_trajectory.py\n",
    "import math\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class SyntheticTrajectoryGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        start_lat: float = 56.0,\n",
    "        start_lng: float = 8.0,\n",
    "        start_timestamp: int = None,\n",
    "        step_seconds: int = 60,\n",
    "        num_points: int = 20,\n",
    "        delta_lat: float = 0.001,\n",
    "        delta_lng: float = 0.001,\n",
    "        noise: float = 0.0,\n",
    "            enable_turns: bool = True,\n",
    "        jitter_time: bool = False,\n",
    "        seed: int = None,\n",
    "    ):\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        self.start_lat = start_lat\n",
    "        self.start_lng = start_lng\n",
    "        self.step_seconds = step_seconds\n",
    "        self.num_points = num_points\n",
    "        self.delta_lat = delta_lat\n",
    "        self.delta_lng = delta_lng\n",
    "        self.noise = noise\n",
    "        self.jitter_time = jitter_time\n",
    "        self.enable_turns = enable_turns\n",
    "\n",
    "        if start_timestamp is None:\n",
    "            import datetime\n",
    "\n",
    "            start_timestamp = int(\n",
    "                datetime.datetime.now(datetime.timezone.utc).timestamp()\n",
    "            )\n",
    "        self.start_timestamp = start_timestamp\n",
    "\n",
    "    def generate(self) -> List[Tuple[float, float, int]]:\n",
    "        lat, lng = self.start_lat, self.start_lng\n",
    "        timestamp = self.start_timestamp\n",
    "\n",
    "        trajectory = []\n",
    "\n",
    "        num_turns = random.randint(1, 5) if self.enable_turns else 0\n",
    "        turning_points = sorted(random.sample(range(2, self.num_points - 2), num_turns))\n",
    "\n",
    "        delta_lat = self.delta_lat\n",
    "        delta_lng = self.delta_lng\n",
    "\n",
    "        for i in range(self.num_points):\n",
    "            # Apply turning point by changing direction randomly\n",
    "            if i in turning_points:\n",
    "                angle = random.uniform(-1.5, 1.5)  # radians\n",
    "                cos_a = math.cos(angle)\n",
    "                sin_a = math.sin(angle)\n",
    "                new_delta_lat = cos_a * delta_lat - sin_a * delta_lng\n",
    "                new_delta_lng = sin_a * delta_lat + cos_a * delta_lng\n",
    "                delta_lat, delta_lng = new_delta_lat, new_delta_lng\n",
    "\n",
    "            # Add noise and jitter\n",
    "            noise_lat = random.uniform(-self.noise, self.noise)\n",
    "            noise_lng = random.uniform(-self.noise, self.noise)\n",
    "            time_offset = random.randint(1, self.step_seconds - 1) if self.jitter_time else self.step_seconds\n",
    "\n",
    "            trajectory.append(\n",
    "                (lng + noise_lng, lat + noise_lat, timestamp + time_offset)\n",
    "            )\n",
    "\n",
    "            lat += delta_lat\n",
    "            lng += delta_lng\n",
    "            timestamp += time_offset\n",
    "\n",
    "        return trajectory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T00:28:28.766971Z",
     "start_time": "2025-11-03T00:28:28.758314Z"
    }
   },
   "source": [
    "# dataset.py\n",
    "import random\nimport time\nimport torch\nfrom torch import Tensor\nfrom torch.utils.data import Dataset\n\n\nclass SyntheticTrajectoryDataset(Dataset):\n    \"\"\"\n    Synthetic trajectory dataset for delta prediction.\n\n    Each trajectory is generated using randomized starting coordinates, timestamps,\n    and motion parameters. The dataset provides spatial and temporal feature tensors\n    as inputs and delta (Δlat, Δlng) targets for supervised learning.\n\n    :param num_trajectories: Number of synthetic trajectories to generate.\n    :param center_lat: Reference latitude for the generation area.\n    :param center_lng: Reference longitude for the generation area.\n    \"\"\"\n\n    def __init__(self, num_trajectories: int, center_lat: float = 56.0, center_lng: float = 8.0):\n        self.trajs: list[tuple[Tensor, Tensor]] = []\n        self.labels: list[Tensor] = []\n        self.original_trajs: list[list[tuple[float, float, int]]] = []\n\n        for _ in range(num_trajectories):\n            seed = random.randint(0, int(1e6))\n            random.seed(seed)\n\n            generator = SyntheticTrajectoryGenerator(\n                start_lat=random.uniform(55.5, 56.5),\n                start_lng=random.uniform(7.5, 8.5),\n                start_timestamp=int(time.time()) + random.randint(-1000, 1000),\n                step_seconds=60,\n                num_points=random.randint(10, 49),\n                delta_lat=random.uniform(-0.001, 0.001),\n                enable_turns=True,\n                delta_lng=random.uniform(-0.001, 0.001),\n                noise=random.uniform(0.0, 0.0001),\n                jitter_time=True,\n                seed=seed,\n            )\n\n            traj = generator.generate()\n            coords = torch.tensor([[p[0], p[1]] for p in traj], dtype=torch.float32)\n            times = torch.tensor([p[2] for p in traj], dtype=torch.float64)\n            temporal_tensor = extract_temporal_features(times[:-1])\n            spatial_deltas = (coords[1:] - coords[:-1]) * 1000\n            temporal_deltas = (times[1:] - times[:-1]) / 60\n            temporal_deltas = temporal_deltas.to(torch.float32)\n            deltas = torch.cat([spatial_deltas, temporal_deltas.unsqueeze(-1)], dim=-1)\n            spatial_tensor = extract_spatial_features(coords[:-1])\n            assert len(spatial_tensor) == len(deltas)\n\n            self.trajs.append((spatial_tensor, temporal_tensor))\n            self.labels.append(deltas)\n            self.original_trajs.append(traj)\n\n    def __len__(self) -> int:\n        \"\"\"\n        :return: Number of trajectories in the dataset.\n        \"\"\"\n        return len(self.trajs)\n\n    def __getitem__(self, idx: int) -> tuple[list[list[tuple[float, float, int]]], tuple[Tensor, Tensor], Tensor]:\n        \"\"\"\n        :param idx: Index of the trajectory sample to retrieve.\n        :return: ((spatial_tensor, temporal_tensor), delta_tensor)\n        \"\"\"\n        spatial, temporal = self.trajs[idx]\n        deltas = self.labels[idx]\n        return self.original_trajs, (spatial, temporal), deltas\n\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# train.py\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Collate function\n",
    "# ============================================================\n",
    "\n",
    "def collate_batch(batch: list[tuple[list, tuple[Tensor, Tensor], Tensor]]) -> tuple[list, Tensor, Tensor, Tensor]:\n",
    "    \"\"\"Pad variable-length trajectories and stack into [T, B, D] batches.\"\"\"\n",
    "    original, spatial_seqs, temporal_seqs, delta_seqs = zip(*[(t, s, t, d) for (t, (s, t), d) in batch])\n",
    "    max_len = max(seq.shape[0] for seq in spatial_seqs)\n",
    "\n",
    "    def pad_sequence(seq_list: list[Tensor]) -> Tensor:\n",
    "        padded = [\n",
    "            torch.cat([seq, torch.zeros(max_len - seq.shape[0], seq.shape[1])], dim=0)\n",
    "            for seq in seq_list\n",
    "        ]\n",
    "        return torch.stack(padded, dim=1)  # [T, B, D]\n",
    "\n",
    "    spatial_batch = pad_sequence(spatial_seqs)\n",
    "    temporal_batch = pad_sequence(temporal_seqs)\n",
    "    delta_batch = pad_sequence(delta_seqs)\n",
    "    return original, spatial_batch, temporal_batch, delta_batch\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Autoregressive prediction\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_autoregressive(\n",
    "    model,\n",
    "    init_coords: torch.Tensor,   # [T₀, 2]\n",
    "    init_times: torch.Tensor,    # [T₀]\n",
    "    num_future_steps: int,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Autoregressively predicts future trajectory points given an initial seed.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    coords = init_coords.clone().to(device)\n",
    "    times = init_times.clone().to(device)\n",
    "\n",
    "    for step in range(num_future_steps):\n",
    "        # 1️⃣ Extract features for current sequence\n",
    "        spatial_feats = extract_spatial_features(coords).unsqueeze(1).to(device)\n",
    "        temporal_feats = extract_temporal_features(times).unsqueeze(1).to(device)\n",
    "\n",
    "        # max 49 points for the model\n",
    "        if spatial_feats.shape[0] > 49:\n",
    "            spatial_feats = spatial_feats[-49:, :, :]\n",
    "            temporal_feats = temporal_feats[-49:, :, :]\n",
    "\n",
    "        # 2️⃣ Predict next-step delta\n",
    "        preds = model(spatial_feats, temporal_feats)  # [T, 1, D_out]\n",
    "        next_delta = preds[-1, 0]                     # [D_out] (still on device)\n",
    "\n",
    "        dlat, dlng, dt = next_delta\n",
    "        dlat, dlng = dlat / 1000.0, dlng / 1000.0  # scale back\n",
    "        dlat = dlat\n",
    "        dlng = dlng\n",
    "        dt = dt * 60.0                         # scale back to seconds\n",
    "        next_t = times[-1] + dt\n",
    "        # 3️⃣ Append predicted point (everything stays on device)\n",
    "        next_coord = coords[-1] + torch.tensor([dlat, dlng], device=device)\n",
    "        coords = torch.cat([coords, next_coord.unsqueeze(0)], dim=0)\n",
    "        times = torch.cat([times, next_t.unsqueeze(0)], dim=0)\n",
    "\n",
    "\n",
    "    return coords, times\n",
    "\n",
    "# ============================================================\n",
    "# Plotting function\n",
    "# ============================================================\n",
    "\n",
    "def plot_trajectories(true_coords: torch.Tensor, pred_coords: torch.Tensor, epoch: int):\n",
    "        true_coords = true_coords.cpu().numpy()\n",
    "        pred_coords = pred_coords.cpu().numpy()\n",
    "\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.plot(true_coords[:, 0], true_coords[:, 1], \"b.-\", label=\"True\")\n",
    "        plt.plot(pred_coords[len(true_coords)-1::, 0, ], pred_coords[len(true_coords)-1:, 1, ], \"orange\", label=\"Predicted\")\n",
    "        plt.scatter(true_coords[0, 0], true_coords[0, 1], c=\"green\", label=\"Start\", zorder=5)\n",
    "        plt.scatter(pred_coords[-1, 0], pred_coords[-1, 1], c=\"red\", label=\"Pred End\", zorder=5)\n",
    "        plt.title(f\"Autoregressive Prediction — Epoch {epoch+1}\")\n",
    "        plt.xlabel(\"Latitude\")\n",
    "        plt.ylabel(\"Longitude\")\n",
    "        plt.legend()\n",
    "        plt.axis(\"equal\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Training function\n",
    "# ============================================================\n",
    "def train_model(\n",
    "    num_epochs: int = 20,\n",
    "    batch_size: int = 8,\n",
    "    learning_rate: float = 1e-4,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ") -> CheminTF:\n",
    "    \"\"\"\n",
    "    Train CheminTF to predict spatial deltas (Δlat, Δlng, Δt) from spatio-temporal features.\n",
    "    Adds a validation set and uses a validation trajectory for plotting.\n",
    "    \"\"\"\n",
    "    # ---------------------- Dataset split ----------------------\n",
    "    full_dataset = SyntheticTrajectoryDataset(num_trajectories=100000)\n",
    "    train_size = int(0.9 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "    # ---------------------- Model setup ----------------------\n",
    "    model = CheminTF(n_heads=4, num_layers=2).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # ---------------------- Metric storage ----------------------\n",
    "    train_losses, val_losses, val_rmses = [], [], []\n",
    "\n",
    "    *_, deltas_ref = val_dataset[0]\n",
    "\n",
    "\n",
    "    # ---------------------- Training loop ----------------------\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for _, spatial, temporal, deltas in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            spatial, temporal, deltas = spatial.to(device), temporal.to(device), deltas.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(spatial, temporal)\n",
    "            loss = criterion(preds, deltas)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # ---------------------- Validation loop ----------------------\n",
    "        model.eval()\n",
    "        val_loss, val_rmse = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for _, spatial, temporal, deltas in val_loader:\n",
    "                spatial, temporal, deltas = spatial.to(device), temporal.to(device), deltas.to(device)\n",
    "                preds = model(spatial, temporal)\n",
    "                loss = criterion(preds, deltas)\n",
    "                val_loss += loss.item()\n",
    "                val_rmse += torch.sqrt(torch.mean((preds - deltas) ** 2)).item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_rmse = val_rmse / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_rmses.append(avg_val_rmse)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d} | Train Loss: {avg_train_loss:.6f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.6f} | Val RMSE: {avg_val_rmse:.4f}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            coord = val_dataset[0][0][0]  # original trajectory\n",
    "            spatial_ref = Tensor(Tensor(coord)[:, :2])\n",
    "            # temporal ref must be float64\n",
    "            temporal_ref = Tensor(Tensor(coord)[:, 2]).to(torch.float64)\n",
    "\n",
    "            pred_coords, pred_times = predict_autoregressive(\n",
    "                model,\n",
    "                init_coords=spatial_ref,\n",
    "                init_times=temporal_ref,\n",
    "                num_future_steps=10,\n",
    "                device=device,\n",
    "            )\n",
    "            plot_trajectories(spatial_ref, pred_coords, epoch)\n",
    "\n",
    "    # ---------------------- Plot training & validation curves ----------------------\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", linewidth=2)\n",
    "    plt.plot(val_losses, label=\"Val Loss\", linewidth=2)\n",
    "    plt.plot(val_rmses, label=\"Val RMSE\", linewidth=2)\n",
    "    plt.title(\"Training & Validation Metrics\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Entry point\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model = train_model(num_epochs=100, batch_size=128, device=\"cuda\")\n",
    "    torch.save(trained_model.state_dict(), \"../weights/chemin_tf_deltas.pt\")\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# evaluate.py\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def evaluate_and_plot(\n",
    "    weights_path: str = \"chemin_tf_deltas.pt\",\n",
    "    num_samples: int = 10,\n",
    "    num_trajectories: int = 1000,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Load pretrained CheminTF model and plot autoregressive trajectory predictions.\n",
    "\n",
    "    Args:\n",
    "        weights_path: Path to model checkpoint (.pt file)\n",
    "        num_samples: Number of trajectories to predict and plot\n",
    "        num_trajectories: Number of synthetic trajectories in dataset\n",
    "        device: \"cuda\" or \"cpu\"\n",
    "    \"\"\"\n",
    "    # ============================================================\n",
    "    # Load model and weights\n",
    "    # ============================================================\n",
    "    model = CheminTF(n_heads=4, num_layers=2).to(device)\n",
    "    model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # ============================================================\n",
    "    # Prepare validation data\n",
    "    # ============================================================\n",
    "    dataset = SyntheticTrajectoryDataset(num_trajectories=num_trajectories)\n",
    "    val_loader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate_batch)\n",
    "    val_iter = iter(val_loader)\n",
    "\n",
    "    # ============================================================\n",
    "    # Predict and plot samples\n",
    "    # ============================================================\n",
    "    for i in range(num_samples):\n",
    "        original, spatial, temporal, delta = next(val_iter)\n",
    "        spatial_ref = spatial.squeeze(1).to(device)\n",
    "        temporal_ref = temporal.squeeze(1).to(device)\n",
    "        coords_ref = spatial_ref[:, :2]\n",
    "        times_ref = temporal_ref[:, 0] if temporal_ref.ndim > 1 else temporal_ref\n",
    "        times_ref = times_ref.to(torch.float64)\n",
    "        pred_coords, pred_times = predict_autoregressive(\n",
    "            model,\n",
    "            init_coords=coords_ref,\n",
    "            init_times=times_ref,\n",
    "            num_future_steps=5,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        plot_trajectories(coords_ref, pred_coords, i)\n",
    "        print(f\"Trajectory {i+1}/{num_samples} plotted ✅\")\n",
    "\n",
    "    print(\"✅ Evaluation and plotting complete.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Example usage\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_and_plot(\"../weights/chemin_tf_deltas.pt\", num_samples=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
