{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Building a trajectory FM from scratch - Part 1: GPT-2\n",
        "\n",
        "This tutorial aims at building an educational foundational model for trajectories. As a first step, let's make sure the environment is set up correctly."
      ],
      "metadata": {
        "id": "_8EKlfks5zei"
      },
      "id": "_8EKlfks5zei"
    },
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2025-08-21T08:50:46.349983Z",
          "start_time": "2025-08-21T08:50:44.032626Z"
        },
        "id": "initial_id"
      },
      "source": [
        "import torch\n",
        "\n",
        "# check if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    raise RuntimeError(\"GPU is not available. Please check your setup. On Google Colab, click on 'Runtime' -> 'Change runtime type' and select 'GPU'.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The codes presented in this notebook are adapted versions of the ones found in the [Building and LLM from Scratch](https://github.com/rasbt/LLMs-from-scratch/tree/main) repository accompagning the corresponding recommended book:\n",
        "\n",
        "> Raschka, Sebastian. Build A Large Language Model (From Scratch). Manning, 2024. ISBN: 978-1633437166.\n",
        "\n",
        "These codes are simplified to correspond to the ones presented in the first part of the presentation:\n",
        "\n",
        "> Building a foundation model for trajectory from scratch.\n",
        "\n",
        "These simplifications are intended to focus on the element of GPT2-like architectures that will be re-used to build trajectory foundational models.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vt9y-ypgzlAR"
      },
      "id": "vt9y-ypgzlAR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Tokenization\n",
        "\n",
        "GPT-2 uses a predifining tokenization algorithm called BPE. While this algorithm has been trained specifically for GPT-2, it is not a part of it.\n"
      ],
      "metadata": {
        "id": "BSuolt6r7lhe"
      },
      "id": "BSuolt6r7lhe"
    },
    {
      "metadata": {
        "id": "1a4229e2661cc933",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24d1d25d-16f1-4ba8-99e3-056434c357de"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[262, 4171, 3797, 26172, 262]\n",
            "[52, 42072, 306, 11, 39155, 14434, 11798, 8902]\n"
          ]
        }
      ],
      "execution_count": 3,
      "source": [
        "############ Code Snippet 1 Tokenization #########################\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "text = \" the blue cat chased the\"\n",
        "token_ids = tokenizer.encode(text)\n",
        "print(token_ids)\n",
        "\n",
        "\n",
        "text2 = \"Unexpectedly, globalization transformed industries rapidly\"\n",
        "token_ids2 = tokenizer.encode(text2)\n",
        "print(token_ids2)\n"
      ],
      "id": "1a4229e2661cc933"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Vector and Positional Embeddings\n",
        "\n",
        "From here, we start building some pytorch modules. An example of custom pytorch module is provided below.\n"
      ],
      "metadata": {
        "id": "DicLAW8O1pQY"
      },
      "id": "DicLAW8O1pQY"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class CustomModule(nn.Module):\n",
        "    def __init__(self, model_input_dimension, model_output_dimension):\n",
        "        super().__init__()\n",
        "\n",
        "        self.example_layer = nn.Linear(model_input_dimension, model_output_dimension)\n",
        "        # ...\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.example_layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "module = CustomModule(model_input_dimension=10, model_output_dimension=15)\n",
        "\n",
        "res = module(torch.rand(10))\n",
        "# res = module(\"test\")\n",
        "print(res)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nbgkynye7k9T",
        "outputId": "7b8cc1a7-004b-4b40-a250-f02680763127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.3725,  0.0729, -0.6745, -0.1772,  0.5579, -0.1531,  0.0535,  0.0891,\n",
            "        -0.4936,  0.0680,  0.1850,  0.4981, -0.0554, -0.2363, -0.2093],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "id": "Nbgkynye7k9T"
    },
    {
      "cell_type": "code",
      "source": [
        "######## Embeddings #######################\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "class EmbeddingLayer(nn.Module):\n",
        "    def __init__(self, d_emb=768, context_size=1024):\n",
        "        super().__init__()\n",
        "        self.vocab_size = 50257  # since we use an already trained tokenizer\n",
        "\n",
        "\n",
        "        self.token_emb_layer = nn.Embedding(self.vocab_size, d_emb)\n",
        "        self.pos_emb_layer = nn.Embedding(context_size, d_emb)\n",
        "\n",
        "    def forward (self, input_ids):\n",
        "        token_embeddings = self.token_emb_layer(input_ids)\n",
        "\n",
        "        num_tokens = input_ids.shape[0]\n",
        "        pos_embeddings = self.pos_emb_layer(torch.arange(num_tokens))   # torch.arange(n) -> [0, 1, 2 ... n]\n",
        "        return token_embeddings + pos_embeddings\n",
        "\n",
        "token_ids = torch.tensor(token_ids)  #\" the blue cat chased the\"\n",
        "embedding_layer = EmbeddingLayer()\n",
        "res = embedding_layer(token_ids)\n",
        "print(res) # careful: pytorch conventions is that embeddings are rows and not columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psuCgBlaX0E9",
        "outputId": "ac1cd742-3ee2-417e-97d1-f45b89469af6"
      },
      "id": "psuCgBlaX0E9",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.1446,  0.0382,  1.7016,  ..., -1.0376,  1.3800,  3.2246],\n",
            "        [-1.1939,  0.7452,  1.5457,  ...,  2.1210, -1.8930, -0.8815],\n",
            "        [ 2.0567,  0.3831,  2.4507,  ..., -1.0001,  2.0558, -2.4634],\n",
            "        [-1.6285, -2.4031, -0.5854,  ...,  2.6113,  0.8406,  1.4919],\n",
            "        [-1.7573,  0.4218, -0.1957,  ..., -0.3381,  0.2605,  1.5082]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. The Transformer Block\n",
        "###4.1 Multi-head Attention\n",
        "\n",
        "We present here an intuitive/naive implementation of multi-head attention\n",
        "consisting ofmultiple single Causal Attention modules in ou multi-headed attention module.\n",
        "\n",
        "This leads to the creation of one wK, wQ and wV with a reduced dimension in each head. This is not optimal. It is indeed possible to declare unique bigger matrices wK, wQ and wV and to use distinct a parts (views) of them to represent the single attention heads.\n",
        "More efficient implementations can be obtained by further exploiting matrix multiplication properties. This goes behind the scope of this tutorial but can be find in the github repository of Sebastia Raschka."
      ],
      "metadata": {
        "id": "8PPJFzQ7_J4f"
      },
      "id": "8PPJFzQ7_J4f"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class CausalAttention(nn.Module):\n",
        "    def __init__(self, d_emb, d_QK, d_head, context_length):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.W_query = nn.Linear(d_emb, d_QK, bias=False)\n",
        "        self.W_key = nn.Linear(d_emb, d_QK, bias=False)\n",
        "        self.W_value = nn.Linear(d_emb, d_head, bias=False)\n",
        "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length,\n",
        "                                                           context_length),\n",
        "                                                diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_emb = x.shape\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2) # (Q@K^T)ij​=Qi​⋅Kj (Qi is row i, Kj is column j)\n",
        "        attn_scores.masked_fill_(\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_embedding, d_QK, context_length, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_embedding % num_heads == 0, \"d_emb must be divisible by num_heads\"\n",
        "        d_head = d_embedding // num_heads  # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CausalAttention(d_embedding, d_QK, d_head, context_length)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)\n"
      ],
      "metadata": {
        "id": "d9JUR9Fya5mY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "d9JUR9Fya5mY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.2 Feed Forward Network and full Transformer\n",
        "\n",
        "First, we will define a few helpers modules which are mentionned but not detailed in the tutorial."
      ],
      "metadata": {
        "id": "L14fqnmg2OwB"
      },
      "id": "L14fqnmg2OwB"
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UzhrqQMWMkKk"
      },
      "id": "UzhrqQMWMkKk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(emb_dim, 4 * emb_dim),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * emb_dim, emb_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_embedding=cfg[\"emb_dim\"],\n",
        "            d_QK=cfg[\"QK_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"])\n",
        "        self.ff = FeedForward(cfg[\"emb_dim\"])\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed-forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ezuCAjdIB5Nt"
      },
      "id": "ezuCAjdIB5Nt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest logits value\n",
        "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9FzMHU1xM_iv"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9FzMHU1xM_iv"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def main():\n",
        "    GPT_CONFIG_124M = {\n",
        "        \"vocab_size\": 50257,     # Vocabulary size\n",
        "        \"context_length\": 4,     # Context length\n",
        "        \"QK_dim\": 64,\n",
        "        \"emb_dim\": 768,          # Embedding dimension\n",
        "        \"n_heads\": 1,           # Number of attention heads\n",
        "        \"n_layers\": 12,          # Number of layers\n",
        "    }\n",
        "\n",
        "    torch.manual_seed(123)\n",
        "    model = GPTModel(GPT_CONFIG_124M)\n",
        "    # model.eval()  # disable dropout\n",
        "\n",
        "    start_context = \"Hello, I am\"\n",
        "\n",
        "    # tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    encoded = tokenizer.encode(start_context)\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "\n",
        "    print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
        "    print(\"\\nInput text:\", start_context)\n",
        "    print(\"Encoded input text:\", encoded)\n",
        "    print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
        "\n",
        "    out = generate_text_simple(\n",
        "        model=model,\n",
        "        idx=encoded_tensor,\n",
        "        max_new_tokens=10,\n",
        "        context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        "    )\n",
        "    decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
        "\n",
        "    print(f\"\\n\\n{50*'='}\\n{22*' '}OUT\\n{50*'='}\")\n",
        "    print(\"\\nOutput:\", out)\n",
        "    print(\"Output length:\", len(out[0]))\n",
        "    print(\"Output text:\", decoded_text)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRql3e5J8LFC",
        "outputId": "1f573f20-e20f-4946-e0ba-83b5e0012fb2"
      },
      "id": "LRql3e5J8LFC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "                      IN\n",
            "==================================================\n",
            "\n",
            "Input text: Hello, I am\n",
            "Encoded input text: [15496, 11, 314, 716]\n",
            "encoded_tensor.shape: torch.Size([1, 4])\n",
            "\n",
            "\n",
            "==================================================\n",
            "                      OUT\n",
            "==================================================\n",
            "\n",
            "Output: tensor([[15496,    11,   314,   716, 17307, 28937, 27997, 49120, 36594, 43496,\n",
            "         43496, 43496, 46743, 20920]])\n",
            "Output length: 14\n",
            "Output text: Hello, I amalphCook fats Glacier pouch Lumin Lumin Lumin scraping Curry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More optimized code from Building an LLM from scratch"
      ],
      "metadata": {
        "id": "rLPEzkRaNB-C"
      },
      "id": "rLPEzkRaNB-C"
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# Optimized does not mean fully optimal, but its a improved/more general version than above\n",
        "class MatrixViewMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.out_proj = nn.Linear(d_out, d_out, bias=False)  # Linear layer to combine head outputs\n",
        "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)  # optional projection\n",
        "\n",
        "        return context_vec\n"
      ],
      "metadata": {
        "id": "vevQJGValy6a"
      },
      "id": "vevQJGValy6a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "# Chapter 4\n",
        "#####################################\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MatrixViewMultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed-forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EmuRebL1hrsf"
      },
      "id": "EmuRebL1hrsf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Full GPT-2 implementation and example usage\n",
        "\n",
        "Now that we saw all necessary building blocks, we will combine them in a GPT-2 class that we can use for inference.\n",
        "\n",
        "Obviously, with"
      ],
      "metadata": {
        "id": "HHENGWk_CwOY"
      },
      "id": "HHENGWk_CwOY"
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest logits value\n",
        "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "def main():\n",
        "    GPT_CONFIG_124M = {\n",
        "        \"vocab_size\": 50257,     # Vocabulary size\n",
        "        \"context_length\": 1024,  # Context length\n",
        "        \"emb_dim\": 768,          # Embedding dimension # To update!\n",
        "        \"n_heads\": 12,           # Number of attention heads\n",
        "        \"n_layers\": 12,          # Number of layers\n",
        "    }\n",
        "\n",
        "    torch.manual_seed(123)\n",
        "    model = GPTModel(GPT_CONFIG_124M)\n",
        "    model.eval()  # disable dropout\n",
        "\n",
        "    start_context = \"Hello, I am\"\n",
        "\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    encoded = tokenizer.encode(start_context)\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "\n",
        "    print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
        "    print(\"\\nInput text:\", start_context)\n",
        "    print(\"Encoded input text:\", encoded)\n",
        "    print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
        "\n",
        "    out = generate_text_simple(\n",
        "        model=model,\n",
        "        idx=encoded_tensor,\n",
        "        max_new_tokens=10,\n",
        "        context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        "    )\n",
        "    decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
        "\n",
        "    print(f\"\\n\\n{50*'='}\\n{22*' '}OUT\\n{50*'='}\")\n",
        "    print(\"\\nOutput:\", out)\n",
        "    print(\"Output length:\", len(out[0]))\n",
        "    print(\"Output text:\", decoded_text)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "bmJkF_5IkpY1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a95680da-a1d2-4176-ddb6-7c342dfb8c53"
      },
      "id": "bmJkF_5IkpY1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "                      IN\n",
            "==================================================\n",
            "\n",
            "Input text: Hello, I am\n",
            "Encoded input text: [15496, 11, 314, 716]\n",
            "encoded_tensor.shape: torch.Size([1, 4])\n",
            "\n",
            "\n",
            "==================================================\n",
            "                      OUT\n",
            "==================================================\n",
            "\n",
            "Output: tensor([[15496,    11,   314,   716, 12192, 17592, 25063, 24649, 44611, 48509,\n",
            "         42730,  8186, 14614, 34476]])\n",
            "Output length: 14\n",
            "Output text: Hello, I amoyd prone solicit memoir Sven Hattaughlin reprodu technological sugars\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}